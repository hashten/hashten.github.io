<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>LSD-SLAM: LSD-SLAM: Large-Scale Direct Monocular SLAM</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">LSD-SLAM
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,'Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('md_README.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">LSD-SLAM: Large-Scale Direct Monocular SLAM </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>LSD-SLAM is a novel approach to real-time monocular SLAM. It is fully direct (i.e. does not use keypoints / features) and creates large-scale, semi-dense maps in real-time on a laptop. For more information see <a href="http://vision.in.tum.de/lsdslam">http://vision.in.tum.de/lsdslam</a> where you can also find the corresponding publications and Youtube videos, as well as some example-input datasets, and the generated output as rosbag or .ply point cloud.</p>
<h3>Related Papers</h3>
<ul>
<li><b>LSD-SLAM: Large-Scale Direct Monocular SLAM</b>, <em>J. Engel, T. Schöps, D. Cremers</em>, ECCV '14</li>
<li><b>Semi-Dense Visual Odometry for a Monocular Camera</b>, <em>J. Engel, J. Sturm, D. Cremers</em>, ICCV '13</li>
</ul>
<h1>1. Quickstart / Minimal Setup</h1>
<p>First, install LSD-SLAM following 2.1 or 2.2, depending on your Ubuntu / ROS version. You don't need openFabMap for now.</p>
<p>Download the <a href="http://vmcremers8.informatik.tu-muenchen.de/lsd/LSD_room.bag.zip">Room Example Sequence</a> and extract it.</p>
<p>Launch the lsd_slam viewer: </p><pre class="fragment">    rosrun lsd_slam_viewer viewer
</pre><p>Launch the lsd_slam main ros node: </p><pre class="fragment">    rosrun lsd_slam_core live_slam image:=/image_raw camera_info:=/camera_info
</pre><p>Play the sequence: </p><pre class="fragment">    rosbag play ~/LSD_room.bag
</pre><p>You should see one window showing the current keyframe with color-coded depth (from live_slam), and one window showing the 3D map (from viewer). If for some reason the initialization fails (i.e., after ~5s the depth map still looks wrong), focus the depth map and hit 'r' to re-initialize.</p>
<h1>2. Installation</h1>
<p>We tested LSD-SLAM on two different system configurations, using Ubuntu 12.04 (Precise) and ROS fuerte, or Ubuntu 14.04 (trusty) and ROS indigo. Note that building without ROS is not supported, however ROS is only used for input and output, facilitating easy portability to other platforms.</p>
<h2>2.1 ROS fuerte + Ubuntu 12.04</h2>
<p>Install system dependencies: </p><pre class="fragment">sudo apt-get install ros-fuerte-libg2o liblapack-dev libblas-dev freeglut3-dev libqglviewer-qt4-dev libsuitesparse-dev libx11-dev
</pre><p>In your ROS package path, clone the repository: </p><pre class="fragment">git clone https://github.com/tum-vision/lsd_slam.git lsd_slam
</pre><p>Compile the two package by typing: </p><pre class="fragment">rosmake lsd_slam
</pre><h2>2.2 ROS indigo + Ubuntu 14.04</h2>
<p><b>We do not use catkin, however fortunately old-fashioned CMake-builds are still possible with ROS indigo.</b> For this you need to create a rosbuild workspace (if you don't have one yet), using: </p><pre class="fragment">sudo apt-get install python-rosinstall
mkdir ~/rosbuild_ws
cd ~/rosbuild_ws
rosws init . /opt/ros/indigo
mkdir package_dir
rosws set ~/rosbuild_ws/package_dir -t .
echo "source ~/rosbuild_ws/setup.bash" &gt;&gt; ~/.bashrc
bash
cd package_dir
</pre><p>Install system dependencies: </p><pre class="fragment">sudo apt-get install ros-indigo-libg2o ros-indigo-cv-bridge liblapack-dev libblas-dev freeglut3-dev libqglviewer-dev libsuitesparse-dev libx11-dev
</pre><p>In your ROS package path, clone the repository: </p><pre class="fragment">git clone https://github.com/tum-vision/lsd_slam.git lsd_slam
</pre><p>Compile the two package by typing: </p><pre class="fragment">rosmake lsd_slam
</pre><h2>2.3 openFabMap for large loop-closure detection [optional]</h2>
<p>If you want to use openFABMAP for large loop closure detection, uncomment the following lines in <code>lsd_slam_core/CMakeLists.txt</code> : </p><pre class="fragment">#add_subdirectory(${PROJECT_SOURCE_DIR}/thirdparty/openFabMap)
#include_directories(${PROJECT_SOURCE_DIR}/thirdparty/openFabMap/include)
#add_definitions("-DHAVE_FABMAP")
#set(FABMAP_LIB openFABMAP )
</pre><p><b>Note for Ubuntu 14.04:</b> The packaged OpenCV for Ubuntu 14.04 does not include the nonfree module, which is required for openFabMap (which requires SURF features). You need to get a full version of OpenCV with nonfree module, which is easiest by compiling your own version. We suggest to use the <a href="https://github.com/Itseez/opencv/releases/tag/2.4.8">2.4.8</a> version, to assure compatibility with the current indigo open-cv package.</p>
<h1>3 Usage</h1>
<p>LSD-SLAM is split into two ROS packages, <code>lsd_slam_core</code> and <code>lsd_slam_viewer</code>. <code>lsd_slam_core</code> contains the full SLAM system, whereas <code>lsd_slam_viewer</code> is optionally used for 3D visualization. Please also read <b>General Notes for good results</b> below.</p>
<h2>3.1 <code>lsd_slam_core</code></h2>
<p>We provide two different usage modes, one meant for live-operation (<code>live_slam</code>) using ROS input/output, and one <code>dataset_slam</code> to use on datasets in the form of image files.</p>
<h3>3.1.1 Using <code>live_slam</code></h3>
<p>If you want to directly use a camera. </p><pre class="fragment">rosrun lsd_slam_core live_slam /image:=&lt;yourstreamtopic&gt; /camera_info:=&lt;yourcamera_infotopic&gt;
</pre><p>When using ROS camera_info, only the image dimensions and the <code>K</code> matrix from the camera info messages will be used - hence the video has to be rectified.</p>
<p>Alternatively, you can specify a calibration file using </p><pre class="fragment">rosrun lsd_slam_core live_slam /image:=&lt;yourstreamtopic&gt; _calib:=&lt;calibration_file&gt;
</pre><p>In this case, the camera_info topic is ignored, and images may also be radially distorted. See the Camera Calibration section for details on the calibration file format.</p>
<h3>3.1.2 Using <code>dataset_slam</code></h3>
<pre class="fragment">rosrun lsd_slam_core dataset_slam _files:=&lt;files&gt; _hz:=&lt;hz&gt; _calib:=&lt;calibration_file&gt;
</pre><p>Here, <code>&lt;files&gt;</code> can either be a folder containing image files (which will be sorted alphabetically), or a text file containing one image file per line. <code>&lt;hz&gt;</code> is the framerate at which the images are processed, and <code>&lt;calibration_file&gt;</code> the camera calibration file.</p>
<p>Specify <code>_hz:=0</code> to enable sequential tracking and mapping, i.e. make sure that every frame is mapped properly. Note that while this typically will give best results, it can be much slower than real-time operation.</p>
<h3>3.1.3 Camera Calibration</h3>
<p>LSD-SLAM operates on a pinhole camera model, however we give the option to undistort images before they are being used. You can find some sample calib files in <code>lsd_slam_core/calib</code>.</p>
<h4>Calibration File for FOV camera model:</h4>
<pre class="fragment">fx/width fy/height cx/width cy/height d
in_width in_height
"crop" / "full" / "none" / "e1 e2 e3 e4 0"
out_width out_height
</pre><p>Here, the values in the first line are the camera intrinsics and radial distortion parameter as given by the PTAM cameracalibrator, in_width and in_height is the input image size, and out_width out_height is the desired undistorted image size. The latter can be chosen freely, however 640x480 is recommended as explained in section 3.1.6. The third line specifies how the image is distorted, either by specifying a desired camera matrix in the same format as the first four intrinsic parameters, or by specifying "crop", which crops the image to maximal size while including only valid image pixels.</p>
<h4>Calibration File for Pre-Rectified Images</h4>
<p>This one is without radial distortion correction, as a special case of ATAN camera model but without the computational cost: </p><pre class="fragment">fx/width fy/height cx/width cy/height 0
width height
none
width height
</pre><h4>Calibration File for OpenCV camera model:</h4>
<pre class="fragment">fx fy cx cy k1 k2 p1 p2
inputWidth inputHeight
"crop" / "full" / "none" / "e1 e2 e3 e4 0"
outputWidth outputHeight
</pre><h3>3.1.4 Useful Hotkeys</h3>
<ul>
<li><code>r</code>: Do a full reset</li>
<li><code>d / e</code>: Cycle through debug displays (in particular color-coded variance and color-coded inverse depth).</li>
<li><code>o</code>: Toggle on screen info display</li>
<li><code>m</code>: Save current state of the map (depth &amp; variance) as images to <code>lsd_slam_core/save/</code></li>
<li><code>p</code>: Brute-Force-Try to find new constraints. May improve the map by finding more constraints, but will block mapping for a while.</li>
<li><code>l</code>: Manually indicate that tracking is lost: will stop tracking and mapping, and start the re-localizer.</li>
</ul>
<h3>3.1.5 Parameters (Dynamic Reconfigure)</h3>
<p>A number of things can be changed dynamically, using (for ROS fuerte) </p><pre class="fragment">rosrun dynamic_reconfigure reconfigure_gui 
</pre><p>or (for ROS indigo) </p><pre class="fragment">rosrun rqt_reconfigure rqt_reconfigure
</pre><p>Parameters are split into two parts, ones that enable / disable various sorts of debug output in <code>/LSD_SLAM/Debug</code>, and ones that affect the actual algorithm, in <code>/LSD_SLAM</code>. Note that debug output options from <code>/LSD_SLAM/Debug</code> only work if lsd_slam_core is built with debug info, e.g. with <code>set(ROS_BUILD_TYPE RelWithDebInfo)</code>.</p>
<ul>
<li><code>minUseGrad</code>: [double] Minimal absolute image gradient for a pixel to be used at all. Increase if your camera has large image noise, decrease if you have low image-noise and want to also exploit small gradients.</li>
<li><code>cameraPixelNoise</code>: [double] Image intensity noise used for e.g. tracking weight calculation. Should be set larger than the actual sensor-noise, to also account for noise originating from discretization / linear interpolation.</li>
<li><code>KFUsageWeight</code>: [double] Determines how often keyframes are taken, depending on the overlap to the current keyframe. Larger -&gt; more keyframes.</li>
<li><code>KFDistWeight</code>: [double] Determines how often keyframes are taken, depending on the distance to the current Keyframe. Larger -&gt; more keyframes.</li>
<li><code>doSLAM</code>: [bool] Toggle global mapping component on/off. Only takes effect after a reset.</li>
<li><code>doKFReActivation</code>: [bool] Toggle keyframe re-activation on/off: If close to an existing keyframe, re-activate it instead of creating a new one. If false, the map will continually grow even if the camera moves in a relatively constrained area; If false, the number of keyframes will not grow arbitrarily.</li>
<li><code>doMapping</code>: [bool] Toggle entire keyframe creating / update module on/off: If false, only tracking stays active, which will prevent rapid motion or moving objects from corrupting the map.</li>
<li><code>useFabMap</code>: [bool] Use openFABMAP to find large loop-closures. Only takes effect after a reset, and requires LSD-SLAM to be compiled with FabMap.</li>
<li><code>allowNegativeIdepths</code>: [bool] Allow idepth to be (slightly) negative to avoid introducing a bias for far-away points.</li>
<li><code>useSubpixelStereo</code>: [bool] Compute subpixel-accurate stereo disparity.</li>
<li><code>useAffineLightningEstimation</code>: [bool] EXPERIMENTAL: Correct for global affine intensity changes during tracking. Might help if you have problems with auto-exposure.</li>
<li><code>multiThreading</code>: [bool] Toggle multi-threading of depth map estimation. Disable for less CPU usage, but possibly slightly less quality.</li>
<li><code>maxLoopClosureCandidates</code>: [int] Maximal number of loop-closures that are tracked initially for each new keyframe.</li>
<li><code>loopclosureStrictness</code>: [double] Threshold on reciprocal loop-closure consistency check, to be added to the map. Larger -&gt; more (possibly wrong) loop-closures.</li>
<li><code>relocalizationTH</code>: [double] How good a relocalization-attempt has to be to be accepted. Larger -&gt; more strict.</li>
<li><code>depthSmoothingFactor</code>: [double] How much to smooth the depth map. Larger -&gt; less smoothing.</li>
</ul>
<p>Useful for debug output are:</p>
<ul>
<li><code>plotStereoImages</code>: [bool] Plot searched stereo lines, and color-coded stereo-results. Nice visualization of what's going on, however drastically decreases mapping speed.</li>
<li><code>plotTracking</code>: [bool] Plot final tracking residual. Nice visualization of what's going on, however drastically decreases tracking speed.</li>
<li><code>continuousPCOutput</code>: [bool] Publish current keyframe's point cloud after each update, to be seen in the viewer. Nice visualization, however bad for performance and bandwidth.</li>
</ul>
<h3>3.1.6 General Notes for Good Results</h3>
<ul>
<li>Use a <b>global shutter</b> camera. Using a rolling shutter will lead to inferior results.</li>
<li>Use a lens with a <b>wide field-of-view</b> (we use a 130° fisheye lens).</li>
<li>Use a <b>high framerate</b>, at least 30fps (depending on the movements speed of course). For our experiments, we used between 30 and 60 fps.</li>
<li>We recommend an image resolution of <b>640x480</b>, significantly higher or lower resolutions may require some hard-coded parameters to be adapted.</li>
<li>LSD-SLAM is a monocular SLAM system, and as such cannot estimate the absolute scale of the map. Further it requires <b>sufficient camera translation</b>: Rotating the camera without translating it at the same time will not work. Generally sideways motion is best - depending on the field of view of your camera, forwards / backwards motion is equally good. Rotation around the optical axis does not cause any problems.</li>
<li>During initialization, it is best to move the camera in a circle parallel to the image without rotating it. The scene should contain sufficient structure (intensity gradient at different depths).</li>
<li><b>Adjust</b> <code>minUseGrad</code> <b>and</b> <code>cameraPixelNoise</code> to fit the sensor-noise and intensity contrast of your camera.</li>
<li>If tracking / mapping quality is poor, try decreasing the keyframe thresholds <code>KFUsageWeight</code> and <code>KFDistWeight</code> slightly to generate more keyframes.</li>
<li>Note that LSD-SLAM is very much non-deterministic, i.e. results will be different each time you run it on the same dataset. This is due to parallelism, and the fact that small changes regarding when keyframes are taken will have a huge impact on everything that follows afterwards.</li>
</ul>
<h2>3.2 LSD-SLAM Viewer</h2>
<p>The viewer is only for visualization. It can also be used to output a generated point cloud as .ply. For live operation, start it using </p><pre class="fragment">rosrun lsd_slam_viewer viewer
</pre><p>You can use rosbag to record and re-play the output generated by certain trajectories. Record &amp; playback using </p><pre class="fragment">rosbag record /lsd_slam/graph /lsd_slam/keyframes /lsd_slam/liveframes -o file_pc.bag
rosbag play file_pc.bag
</pre><p>You should never have to restart the viewer node, it resets the graph automatically.</p>
<p>If you just want to lead a certain pointcloud from a .bag file into the viewer, you can directly do that using </p><pre class="fragment">rosrun lsd_slam_viewer viewer file_pc.bag
</pre><h3>3.2.1 Useful Hotkeys</h3>
<ul>
<li><code>r</code>: Reset, will clear all displayed data.</li>
<li><code>w</code>: Print the number of points / currently displayed points / keyframes / constraints to the console.</li>
<li><code>p</code>: Write currently displayed points as point cloud to file lsd_slam_viewer/pc.ply, which can be opened e.g. in meshlab. Use in combination with sparsityFactor to reduce the number of points.</li>
</ul>
<h3>3.2.2 Parameters (Dynamic Reconfigure)</h3>
<ul>
<li><code>showKFCameras</code>: Toggle drawing of blue keyframe camera-frustrums. min: False, default: True, max: True</li>
<li><code>showKFPointclouds</code>: Toggle drawing of point clouds for all keyframes. min: False, default: True, max: True</li>
<li><code>showConstraints</code>: Toggle drawing of red/green pose-graph constraints. min: False, default: True, max: True</li>
<li><code>showCurrentCamera</code>: Toggle drawing of red frustrum for the current camera pose. min: False, default: True, max: True</li>
<li><code>showCurrentPointcloud</code>: Toggle drawing of the latest point cloud added to the map. min: False, default: True, max: True</li>
<li><code>pointTesselation</code>: Size of points. min: 0.0, default: 1.0, max: 5.0</li>
<li><code>lineTesselation</code>: Width of lines. min: 0.0, default: 1.0, max: 5.0</li>
<li><code>scaledDepthVarTH</code>: log10 of threshold on point's variance, in the respective keyframe's scale. min: -10.0, default: -3.0, max: 1.0</li>
<li><code>absDepthVarTH</code>: log10 of threshold on point's variance, in absolute scale. min: -10.0, default: -1.0, max: 1.0</li>
<li><code>minNearSupport</code>: Only plot points that have #minNearSupport similar neighbours (higher values remove outliers). min: 0, default: 7, max: 9</li>
<li><code>cutFirstNKf</code>: Do not display the first #cutFirstNKf keyframe's point clouds, to remove artifacts left-over from the random initialization. min: 0, default: 5, max: 100</li>
<li><code>sparsifyFactor</code>: Only plot one out of #sparsifyFactor points, selected at random. Use this to significantly speed up rendering for large maps. min: 1, default: 1, max: 100</li>
<li><code>sceneRadius</code>: Defines near- and far clipping plane. Decrease to be able to zoom in more. min: 1, default: 80, max: 200</li>
<li><code>saveAllVideo</code>: Save all rendered images... only use if you know what you are doing. min: False, default: False, max: True</li>
<li><code>keepInMemory</code>: If set to false, the point cloud is only stored in OpenGL buffers, and not kept in RAM. This greatly reduces the required RAM for large maps, however also prohibits saving / dynamically changing sparsifyFactor and variance-thresholds. min: False, default: True, max: True</li>
</ul>
<h1>4 Datasets</h1>
<p>For convenience we provide a number of datasets, including the video, lsd-slam's output and the generated point cloud as .ply. See <a href="http://vision.in.tum.de/lsdslam">http://vision.in.tum.de/lsdslam</a></p>
<h1>5 License</h1>
<p>LSD-SLAM is licensed under the GNU General Public License Version 3 (GPLv3), see <a href="http://www.gnu.org/licenses/gpl.html">http://www.gnu.org/licenses/gpl.html</a>.</p>
<p>For commercial purposes, we also offer a professional version under different licencing terms.</p>
<h1>6 Troubleshoot / FAQ</h1>
<p><b>How can I get the live-pointcloud in ROS to use with RVIZ?</b></p>
<p>You cannot, at least not on-line and in real-time. The reason is the following:</p>
<p>In the background, LSD-SLAM continuously optimizes the pose-graph, i.e., the poses of all keyframes. Each time a keyframe's pose changes (which happens all the time, if only by a little bit), all points from this keyframe change their 3D position with it. Hence, you would have to continuously re-publish and re-compute the whole pointcloud (at 100k points per keyframe and up to 1000 keyframes for the longer sequences, that's 100 million points, i.e., ~1.6GB), which would crush real-time performance.</p>
<p>Instead, this is solved in LSD-SLAM by publishing keyframes and their poses separately:</p><ul>
<li>keyframeGraphMsg contains the updated pose of each keyframe, nothing else.</li>
<li>keyframeMsg contains one frame with it's pose, and - if it is a keyframe - it's points in the form of a depth map.</li>
</ul>
<p>Points are then always kept in their keyframe's coodinate system: That way, a keyframe's pose can be changed without even touching the points. In fact, in the viewer, the points in the keyframe's coodinate frame are moved to a GLBuffer immediately and never touched again - the only thing that changes is the pushed modelViewMatrix before rendering.</p>
<p>Note that "pose" always refers to a Sim3 pose (7DoF, including scale) - which ROS doesn't even have a message type for.</p>
<p>If you need some other way in which the map is published (e.g. publish the whole pointcloud as ROS standard message as a service), the easiest is to implement your own Output3DWrapper.</p>
<p><b>Tracking immediately diverges / I keep getting "TRACKING LOST for frame 34 (0.00% good Points, which is -nan% of available points, DIVERGED)!"</b></p><ul>
<li>double-check your camera calibration.</li>
<li>try more translational movement and less roational movement </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.12 </li>
  </ul>
</div>
</body>
</html>
